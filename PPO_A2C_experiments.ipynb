{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_A2C_experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sdJghYQlWsKd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Dg305UJ0sv",
        "outputId": "44990dd9-ca5f-489d-da75-79ea05da373b"
      },
      "source": [
        "!pip install or_gym"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting or_gym\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/df/4d6ac80258329ed1bdeb622754c00150dfed26c50aee7cf6d8e90d3b3d2f/or_gym-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: gym>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from or_gym) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.7/dist-packages (from or_gym) (1.19.5)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.7/dist-packages (from or_gym) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from or_gym) (1.4.1)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from or_gym) (2.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15.0->or_gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15.0->or_gym) (1.5.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->or_gym) (1.3.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.3->or_gym) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.15.0->or_gym) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.1->or_gym) (1.15.0)\n",
            "Installing collected packages: or-gym\n",
            "Successfully installed or-gym-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LRrxR9DJzOr"
      },
      "source": [
        "import or_gym\r\n",
        "from or_gym.utils import create_env, assign_env_config"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X56AnRK51FdQ"
      },
      "source": [
        "import gym\r\n",
        "import math\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchvision.transforms as T\r\n",
        "\r\n",
        "from torch.distributions import MultivariateNormal"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4_DS16u2yLd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "#Transition = namedtuple('Transition',\r\n",
        "#                        ('state', 'action', 'next_state', 'reward'))\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EDX4zXm1JvU"
      },
      "source": [
        "class Memory:\r\n",
        "    def __init__(self):\r\n",
        "        self.actions = []\r\n",
        "        self.states = []\r\n",
        "        self.logprobs = []\r\n",
        "        self.rewards = []\r\n",
        "        self.is_terminals = []\r\n",
        "    \r\n",
        "    def clear_memory(self):\r\n",
        "        del self.actions[:]\r\n",
        "        del self.states[:]\r\n",
        "        del self.logprobs[:]\r\n",
        "        del self.rewards[:]\r\n",
        "        del self.is_terminals[:]\r\n",
        "\r\n",
        "class ActorCritic(nn.Module):\r\n",
        "    def __init__(self, state_dim, action_dim, action_std):\r\n",
        "        self.gamma = 0.99\r\n",
        "        self.K_epochs = 80\r\n",
        "        super(ActorCritic, self).__init__()\r\n",
        "        # action mean range -1 to 1\r\n",
        "        self.actor =  nn.Sequential(\r\n",
        "                nn.Linear(state_dim, 256),\r\n",
        "                nn.ELU(),\r\n",
        "                nn.Linear(256, 256),\r\n",
        "                nn.ELU(),\r\n",
        "                nn.Linear(256, 2*action_dim),\r\n",
        "                nn.ELU()\r\n",
        "                )\r\n",
        "        # critic\r\n",
        "        self.critic = nn.Sequential(\r\n",
        "                nn.Linear(state_dim, 256),\r\n",
        "                nn.ELU(),\r\n",
        "                nn.Linear(256, 256),\r\n",
        "                nn.ELU(),\r\n",
        "                nn.Linear(256, 1)\r\n",
        "                )\r\n",
        "        self.action_var = torch.full((action_dim,), action_std*action_std).to(device)\r\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr, betas=betas)\r\n",
        "        \r\n",
        "    def forward(self):\r\n",
        "        raise NotImplementedError\r\n",
        "    \r\n",
        "    def act(self, state, memory=None):\r\n",
        "        \r\n",
        "        output = self.actor(state)\r\n",
        "       # print(output)\r\n",
        "        action_mean = output[0,:3]\r\n",
        "        cov_mat = torch.diag(output[0,3:]**2).to(device)\r\n",
        "        \r\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\r\n",
        "        action = dist.sample()\r\n",
        "        action_logprob = dist.log_prob(action)\r\n",
        "        if memory:\r\n",
        "            memory.states.append(state)\r\n",
        "            memory.actions.append(action)\r\n",
        "            memory.logprobs.append(action_logprob)\r\n",
        "        \r\n",
        "        return action.detach()\r\n",
        "    \r\n",
        "    def evaluate(self, state, action):   \r\n",
        "        \r\n",
        "\r\n",
        "        output = self.actor(state)\r\n",
        "        action_mean = output[:,:3]\r\n",
        "        cov_mat = torch.diag_embed(output[:,3:]**2).to(device)\r\n",
        "        \r\n",
        "       # action_var = self.action_var.expand_as(action_mean)\r\n",
        "        #cov_mat = torch.diag_embed(action_var).to(device)\r\n",
        "        \r\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\r\n",
        "        \r\n",
        "        action_logprobs = dist.log_prob(action)\r\n",
        "        dist_entropy = dist.entropy()\r\n",
        "        state_value = self.critic(state)\r\n",
        "        \r\n",
        "        return action_logprobs, torch.squeeze(state_value), dist_entropy\r\n",
        "\r\n",
        "    def update(self,memory):\r\n",
        "        rewards = []\r\n",
        "        discounted_reward = 0\r\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\r\n",
        "            if is_terminal:\r\n",
        "                discounted_reward = 0\r\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\r\n",
        "            rewards.insert(0, discounted_reward)\r\n",
        "        \r\n",
        "        #print(rewards)\r\n",
        "        # Normalizing the rewards:\r\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\r\n",
        "        #rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\r\n",
        "        \r\n",
        "        # convert list to tensor\r\n",
        "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\r\n",
        "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\r\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\r\n",
        "\r\n",
        "\r\n",
        "        # Optimize policy for K epochs:\r\n",
        "        for _ in range(self.K_epochs):\r\n",
        "            # Evaluating old actions and values :\r\n",
        "            logprobs, state_values, dist_entropy = self.evaluate(old_states, old_actions)\r\n",
        "            \r\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\r\n",
        "            #ratios = torch.exp(logprobs - old_logprobs.detach())\r\n",
        "\r\n",
        "            advantages = rewards - state_values.detach()\r\n",
        "            action_loss = -(advantages.detach() * logprobs).mean()\r\n",
        "            value_loss = advantages.pow(2).mean()\r\n",
        "            loss = action_loss + value_loss\r\n",
        "\r\n",
        "            # take gradient step\r\n",
        "            self.optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            self.optimizer.step()\r\n",
        "            \r\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-yxotSmlHNF"
      },
      "source": [
        "class A2C: \r\n",
        "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\r\n",
        "        self.lr = lr\r\n",
        "        self.betas = betas\r\n",
        "        self.gamma = gamma\r\n",
        "        self.K_epochs = K_epochs\r\n",
        "        \r\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\r\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\r\n",
        "      \r\n",
        "        self.MseLoss = nn.MSELoss()\r\n",
        "    \r\n",
        "    def select_action(self, state, memory):\r\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\r\n",
        "        return self.policy.act(state, memory).cpu().data.numpy().flatten()\r\n",
        "    \r\n",
        "    def update(self, memory):\r\n",
        "        # Monte Carlo estimate of rewards:\r\n",
        "        rewards = []\r\n",
        "        discounted_reward = 0\r\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\r\n",
        "            if is_terminal:\r\n",
        "                discounted_reward = 0\r\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\r\n",
        "            rewards.insert(0, discounted_reward)\r\n",
        "        \r\n",
        "        #print(rewards)\r\n",
        "        # Normalizing the rewards:\r\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\r\n",
        "        #rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\r\n",
        "        \r\n",
        "        # convert list to tensor\r\n",
        "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\r\n",
        "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\r\n",
        "        \r\n",
        "        # Optimize policy for K epochs:\r\n",
        "        for _ in range(self.K_epochs):\r\n",
        "            # Evaluating old actions and values :\r\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\r\n",
        "            \r\n",
        "            # Finding Surrogate Loss:\r\n",
        "            advantages = rewards - state_values.detach()   \r\n",
        "            action_loss = -(advantages * logprobs).mean()\r\n",
        "            loss = action_loss + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\r\n",
        "            \r\n",
        "            # take gradient step\r\n",
        "            self.optimizer.zero_grad()\r\n",
        "            loss.mean().backward()\r\n",
        "            self.optimizer.step()\r\n",
        "            \r\n",
        "        # Copy new weights into old policy:\r\n",
        "      "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX76akQ35nrf"
      },
      "source": [
        "class PPO:\r\n",
        "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\r\n",
        "        self.lr = lr\r\n",
        "        self.betas = betas\r\n",
        "        self.gamma = gamma\r\n",
        "        self.eps_clip = eps_clip\r\n",
        "        self.K_epochs = K_epochs\r\n",
        "        \r\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\r\n",
        "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\r\n",
        "        \r\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)\r\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\r\n",
        "        \r\n",
        "        self.MseLoss = nn.MSELoss()\r\n",
        "    \r\n",
        "    def select_action(self, state, memory):\r\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\r\n",
        "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\r\n",
        "    \r\n",
        "    def update(self, memory):\r\n",
        "        # Monte Carlo estimate of rewards:\r\n",
        "        rewards = []\r\n",
        "        discounted_reward = 0\r\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\r\n",
        "            if is_terminal:\r\n",
        "                discounted_reward = 0\r\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\r\n",
        "            rewards.insert(0, discounted_reward)\r\n",
        "        \r\n",
        "        #print(rewards)\r\n",
        "        # Normalizing the rewards:\r\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\r\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\r\n",
        "        \r\n",
        "        # convert list to tensor\r\n",
        "        old_states = torch.squeeze(torch.stack(memory.states).to(device), 1).detach()\r\n",
        "        old_actions = torch.squeeze(torch.stack(memory.actions).to(device), 1).detach()\r\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs)).to(device).detach()\r\n",
        "        \r\n",
        "        # Optimize policy for K epochs:\r\n",
        "        for _ in range(self.K_epochs):\r\n",
        "            # Evaluating old actions and values :\r\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\r\n",
        "            \r\n",
        "            # Finding the ratio (pi_theta / pi_theta__old):\r\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\r\n",
        "\r\n",
        "            # Finding Surrogate Loss:\r\n",
        "            advantages = rewards - state_values.detach()   \r\n",
        "            surr1 = ratios * advantages\r\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\r\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\r\n",
        "            \r\n",
        "            # take gradient step\r\n",
        "            self.optimizer.zero_grad()\r\n",
        "            loss.mean().backward()\r\n",
        "            self.optimizer.step()\r\n",
        "            \r\n",
        "        # Copy new weights into old policy:\r\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def train_model(agent):\r\n",
        "\r\n",
        "    memory = Memory()\r\n",
        "    # logging variables\r\n",
        "    reward_history = []\r\n",
        "    running_reward = 0\r\n",
        "    episode_reward = 0\r\n",
        "    avg_length = 0\r\n",
        "    time_step = 0\r\n",
        "    # training loop\r\n",
        "    for i_episode in range(1, max_episodes+1):\r\n",
        "        state = env.reset()\r\n",
        "        for t in range(max_timesteps):\r\n",
        "            time_step +=1\r\n",
        "            # Running policy_old:\r\n",
        "            action = agent.select_action(state, memory)\r\n",
        "            state, reward, done, _ = env.step(action)\r\n",
        "            \r\n",
        "            # Saving reward and is_terminals:\r\n",
        "            memory.rewards.append(reward)\r\n",
        "            memory.is_terminals.append(done)\r\n",
        "            \r\n",
        "            # update if its time\r\n",
        "            if time_step % update_timestep == 0:\r\n",
        "              # print('update')\r\n",
        "                agent.update(memory)\r\n",
        "                memory.clear_memory()\r\n",
        "                time_step = 0\r\n",
        "            running_reward += reward\r\n",
        "            episode_reward += reward\r\n",
        "            if done:\r\n",
        "                break\r\n",
        "        reward_history.append(episode_reward)\r\n",
        "        episode_reward = 0\r\n",
        "        # logging\r\n",
        "        if i_episode % log_interval == 0:\r\n",
        "            running_reward = int((running_reward/log_interval))\r\n",
        "            print('Episode {} \\t Avg reward: {}'.format(i_episode, running_reward))\r\n",
        "            running_reward = 0\r\n",
        "            avg_length = 0\r\n",
        "\r\n",
        "    return reward_history\r\n",
        "               "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR1ffQCjd1ib"
      },
      "source": [
        "env = or_gym.make(\"InvManagement-v1\")\r\n",
        "env_config = {\r\n",
        "    \"L\":[3, 5, 10],\r\n",
        "    \"p\":2\r\n",
        "}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYaSNxZ5XkRQ"
      },
      "source": [
        "assign_env_config(env,env_config)\r\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHSQ_E5EY5GV"
      },
      "source": [
        "solved_reward = 300         # stop training if avg_reward > solved_reward\r\n",
        "log_interval = 50           # print avg reward in the interval\r\n",
        "max_episodes = 10000        # max training episodes\r\n",
        "max_timesteps = 1000        # max timesteps in one episode\r\n",
        "\r\n",
        "update_timestep = 1000      # update policy every n timesteps\r\n",
        "action_std = 0.5            # constant std for action distribution (Multivariate Normal)\r\n",
        "K_epochs = 80               # update policy for K epochs\r\n",
        "eps_clip = 0.3             # clip parameter for PPO\r\n",
        "gamma = 0.99                # discount factor\r\n",
        "\r\n",
        "lr = 3e-5                # parameters for Adam optimizer\r\n",
        "betas = (0.9, 0.999)\r\n",
        "\r\n",
        "state_dim = env.observation_space.shape[0]\r\n",
        "action_dim = env.action_space.shape[0]\r\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQmcO_AZY15N"
      },
      "source": [
        "agent = PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r51hYyuP-Gy",
        "outputId": "f2279af5-103d-4632-9d4a-66e15ab3f874"
      },
      "source": [
        "h = train_model(agent)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 50 \t Avg reward: -277\n",
            "Episode 100 \t Avg reward: -280\n",
            "Episode 150 \t Avg reward: -281\n",
            "Episode 200 \t Avg reward: -282\n",
            "Episode 250 \t Avg reward: -284\n",
            "Episode 300 \t Avg reward: -285\n",
            "Episode 350 \t Avg reward: -282\n",
            "Episode 400 \t Avg reward: -283\n",
            "Episode 450 \t Avg reward: -283\n",
            "Episode 500 \t Avg reward: -284\n",
            "Episode 550 \t Avg reward: -285\n",
            "Episode 600 \t Avg reward: -282\n",
            "Episode 650 \t Avg reward: -284\n",
            "Episode 700 \t Avg reward: -284\n",
            "Episode 750 \t Avg reward: -283\n",
            "Episode 800 \t Avg reward: -282\n",
            "Episode 850 \t Avg reward: -281\n",
            "Episode 900 \t Avg reward: -280\n",
            "Episode 950 \t Avg reward: -282\n",
            "Episode 1000 \t Avg reward: -283\n",
            "Episode 1050 \t Avg reward: -284\n",
            "Episode 1100 \t Avg reward: -282\n",
            "Episode 1150 \t Avg reward: -282\n",
            "Episode 1200 \t Avg reward: -282\n",
            "Episode 1250 \t Avg reward: -280\n",
            "Episode 1300 \t Avg reward: -281\n",
            "Episode 1350 \t Avg reward: -281\n",
            "Episode 1400 \t Avg reward: -281\n",
            "Episode 1450 \t Avg reward: -279\n",
            "Episode 1500 \t Avg reward: -278\n",
            "Episode 1550 \t Avg reward: -278\n",
            "Episode 1600 \t Avg reward: -279\n",
            "Episode 1650 \t Avg reward: -278\n",
            "Episode 1700 \t Avg reward: -277\n",
            "Episode 1750 \t Avg reward: -275\n",
            "Episode 1800 \t Avg reward: -276\n",
            "Episode 1850 \t Avg reward: -278\n",
            "Episode 1900 \t Avg reward: -278\n",
            "Episode 1950 \t Avg reward: -275\n",
            "Episode 2000 \t Avg reward: -278\n",
            "Episode 2050 \t Avg reward: -275\n",
            "Episode 2100 \t Avg reward: -275\n",
            "Episode 2150 \t Avg reward: -274\n",
            "Episode 2200 \t Avg reward: -278\n",
            "Episode 2250 \t Avg reward: -278\n",
            "Episode 2300 \t Avg reward: -278\n",
            "Episode 2350 \t Avg reward: -281\n",
            "Episode 2400 \t Avg reward: -278\n",
            "Episode 2450 \t Avg reward: -278\n",
            "Episode 2500 \t Avg reward: -278\n",
            "Episode 2550 \t Avg reward: -279\n",
            "Episode 2600 \t Avg reward: -276\n",
            "Episode 2650 \t Avg reward: -279\n",
            "Episode 2700 \t Avg reward: -278\n",
            "Episode 2750 \t Avg reward: -280\n",
            "Episode 2800 \t Avg reward: -279\n",
            "Episode 2850 \t Avg reward: -276\n",
            "Episode 2900 \t Avg reward: -276\n",
            "Episode 2950 \t Avg reward: -275\n",
            "Episode 3000 \t Avg reward: -275\n",
            "Episode 3050 \t Avg reward: -276\n",
            "Episode 3100 \t Avg reward: -273\n",
            "Episode 3150 \t Avg reward: -274\n",
            "Episode 3200 \t Avg reward: -272\n",
            "Episode 3250 \t Avg reward: -274\n",
            "Episode 3300 \t Avg reward: -270\n",
            "Episode 3350 \t Avg reward: -270\n",
            "Episode 3400 \t Avg reward: -268\n",
            "Episode 3450 \t Avg reward: -265\n",
            "Episode 3500 \t Avg reward: -264\n",
            "Episode 3550 \t Avg reward: -260\n",
            "Episode 3600 \t Avg reward: -258\n",
            "Episode 3650 \t Avg reward: -249\n",
            "Episode 3700 \t Avg reward: -245\n",
            "Episode 3750 \t Avg reward: -235\n",
            "Episode 3800 \t Avg reward: -234\n",
            "Episode 3850 \t Avg reward: -230\n",
            "Episode 3900 \t Avg reward: -225\n",
            "Episode 3950 \t Avg reward: -221\n",
            "Episode 4000 \t Avg reward: -217\n",
            "Episode 4050 \t Avg reward: -212\n",
            "Episode 4100 \t Avg reward: -212\n",
            "Episode 4150 \t Avg reward: -195\n",
            "Episode 4200 \t Avg reward: -184\n",
            "Episode 4250 \t Avg reward: -176\n",
            "Episode 4300 \t Avg reward: -167\n",
            "Episode 4350 \t Avg reward: -142\n",
            "Episode 4400 \t Avg reward: -128\n",
            "Episode 4450 \t Avg reward: -107\n",
            "Episode 4500 \t Avg reward: -90\n",
            "Episode 4550 \t Avg reward: -70\n",
            "Episode 4600 \t Avg reward: -54\n",
            "Episode 4650 \t Avg reward: -39\n",
            "Episode 4700 \t Avg reward: -23\n",
            "Episode 4750 \t Avg reward: -6\n",
            "Episode 4800 \t Avg reward: 7\n",
            "Episode 4850 \t Avg reward: 24\n",
            "Episode 4900 \t Avg reward: 31\n",
            "Episode 4950 \t Avg reward: 49\n",
            "Episode 5000 \t Avg reward: 57\n",
            "Episode 5050 \t Avg reward: 68\n",
            "Episode 5100 \t Avg reward: 76\n",
            "Episode 5150 \t Avg reward: 86\n",
            "Episode 5200 \t Avg reward: 97\n",
            "Episode 5250 \t Avg reward: 105\n",
            "Episode 5300 \t Avg reward: 118\n",
            "Episode 5350 \t Avg reward: 135\n",
            "Episode 5400 \t Avg reward: 134\n",
            "Episode 5450 \t Avg reward: 140\n",
            "Episode 5500 \t Avg reward: 152\n",
            "Episode 5550 \t Avg reward: 162\n",
            "Episode 5600 \t Avg reward: 141\n",
            "Episode 5650 \t Avg reward: 164\n",
            "Episode 5700 \t Avg reward: 156\n",
            "Episode 5750 \t Avg reward: 164\n",
            "Episode 5800 \t Avg reward: 179\n",
            "Episode 5850 \t Avg reward: 169\n",
            "Episode 5900 \t Avg reward: 176\n",
            "Episode 5950 \t Avg reward: 179\n",
            "Episode 6000 \t Avg reward: 175\n",
            "Episode 6050 \t Avg reward: 143\n",
            "Episode 6100 \t Avg reward: 161\n",
            "Episode 6150 \t Avg reward: 184\n",
            "Episode 6200 \t Avg reward: 190\n",
            "Episode 6250 \t Avg reward: 205\n",
            "Episode 6300 \t Avg reward: 209\n",
            "Episode 6350 \t Avg reward: 195\n",
            "Episode 6400 \t Avg reward: 215\n",
            "Episode 6450 \t Avg reward: 210\n",
            "Episode 6500 \t Avg reward: 210\n",
            "Episode 6550 \t Avg reward: 214\n",
            "Episode 6600 \t Avg reward: 207\n",
            "Episode 6650 \t Avg reward: 221\n",
            "Episode 6700 \t Avg reward: 218\n",
            "Episode 6750 \t Avg reward: 214\n",
            "Episode 6800 \t Avg reward: 216\n",
            "Episode 6850 \t Avg reward: 214\n",
            "Episode 6900 \t Avg reward: 229\n",
            "Episode 6950 \t Avg reward: 232\n",
            "Episode 7000 \t Avg reward: 220\n",
            "Episode 7050 \t Avg reward: 227\n",
            "Episode 7100 \t Avg reward: 230\n",
            "Episode 7150 \t Avg reward: 235\n",
            "Episode 7200 \t Avg reward: 248\n",
            "Episode 7250 \t Avg reward: 232\n",
            "Episode 7300 \t Avg reward: 247\n",
            "Episode 7350 \t Avg reward: 247\n",
            "Episode 7400 \t Avg reward: 261\n",
            "Episode 7450 \t Avg reward: 241\n",
            "Episode 7500 \t Avg reward: 229\n",
            "Episode 7550 \t Avg reward: 245\n",
            "Episode 7600 \t Avg reward: 253\n",
            "Episode 7650 \t Avg reward: 235\n",
            "Episode 7700 \t Avg reward: 250\n",
            "Episode 7750 \t Avg reward: 260\n",
            "Episode 7800 \t Avg reward: 258\n",
            "Episode 7850 \t Avg reward: 263\n",
            "Episode 7900 \t Avg reward: 250\n",
            "Episode 7950 \t Avg reward: 251\n",
            "Episode 8000 \t Avg reward: 252\n",
            "Episode 8050 \t Avg reward: 265\n",
            "Episode 8100 \t Avg reward: 265\n",
            "Episode 8150 \t Avg reward: 269\n",
            "Episode 8200 \t Avg reward: 262\n",
            "Episode 8250 \t Avg reward: 273\n",
            "Episode 8300 \t Avg reward: 261\n",
            "Episode 8350 \t Avg reward: 253\n",
            "Episode 8400 \t Avg reward: 256\n",
            "Episode 8450 \t Avg reward: 261\n",
            "Episode 8500 \t Avg reward: 272\n",
            "Episode 8550 \t Avg reward: 266\n",
            "Episode 8600 \t Avg reward: 271\n",
            "Episode 8650 \t Avg reward: 279\n",
            "Episode 8700 \t Avg reward: 278\n",
            "Episode 8750 \t Avg reward: 275\n",
            "Episode 8800 \t Avg reward: 275\n",
            "Episode 8850 \t Avg reward: 280\n",
            "Episode 8900 \t Avg reward: 286\n",
            "Episode 8950 \t Avg reward: 289\n",
            "Episode 9000 \t Avg reward: 278\n",
            "Episode 9050 \t Avg reward: 269\n",
            "Episode 9100 \t Avg reward: 281\n",
            "Episode 9150 \t Avg reward: 293\n",
            "Episode 9200 \t Avg reward: 293\n",
            "Episode 9250 \t Avg reward: 278\n",
            "Episode 9300 \t Avg reward: 294\n",
            "Episode 9350 \t Avg reward: 297\n",
            "Episode 9400 \t Avg reward: 288\n",
            "Episode 9450 \t Avg reward: 288\n",
            "Episode 9500 \t Avg reward: 296\n",
            "Episode 9550 \t Avg reward: 287\n",
            "Episode 9600 \t Avg reward: 301\n",
            "Episode 9650 \t Avg reward: 295\n",
            "Episode 9700 \t Avg reward: 304\n",
            "Episode 9750 \t Avg reward: 301\n",
            "Episode 9800 \t Avg reward: 295\n",
            "Episode 9850 \t Avg reward: 310\n",
            "Episode 9900 \t Avg reward: 304\n",
            "Episode 9950 \t Avg reward: 309\n",
            "Episode 10000 \t Avg reward: 306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "MWH5NOD7R8E2",
        "outputId": "9a09c86f-dfad-4598-b9fd-e11fdf31f870"
      },
      "source": [
        "plt.figure(figsize=(8,8))\r\n",
        "plt.plot(h)\r\n",
        "plt.xlabel(\"num episodes\",fontsize=16)\r\n",
        "plt.ylabel(\"reward\",fontsize=16)\r\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJghYQlWsKd"
      },
      "source": [
        "## Experiments from Medium"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFo6Djec6HG9",
        "outputId": "aedc8b8d-d0e9-4bab-ab71-b51710328542"
      },
      "source": [
        "!pip install ray==1.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ray==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/56/4ed88c41b90a285f056a0433eca7eaecb1783afea0dc9a601bb46389758f/ray-1.0.0-cp37-cp37m-manylinux1_x86_64.whl (22.9MB)\n",
            "\u001b[K     |████████████████████████████████| 22.9MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (1.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (3.0.12)\n",
            "Collecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/b7/2056a6f06adb93f679f2a1e415dd33219b7c66ba69b8fd2ff1668b8064ed/py_spy-0.3.4-py2.py3-none-manylinux1_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (2.23.0)\n",
            "Collecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 13.5MB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (2.6.0)\n",
            "Collecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/d6/b952f11b29c3a0cbec5620de3c4260cecd8c4329d83e91587edb48691e15/opencensus-0.7.12-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (1.0.2)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (3.13)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a6/d36302eba284f4f427dc288f6b3ecd7f89d739cfca206b80311d3158f6d9/aiohttp-3.7.4-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.1MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (0.9.0)\n",
            "Collecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hCollecting redis<3.5.0,>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (2.0.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray==1.0.0) (1.24.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat->ray==1.0.0) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat->ray==1.0.0) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray==1.0.0) (1.16.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray==1.0.0) (20.3.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->ray==1.0.0) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 25.0MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/09/986288478cd05126c7f8eeec912d051b8e4fa52965d5c26d066d6dbce194/hiredis-1.1.0-cp37-cp37m-manylinux2010_x86_64.whl (62kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from google->ray==1.0.0) (4.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->ray==1.0.0) (54.0.0)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (1.27.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (2018.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (4.2.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray==1.0.0) (0.4.8)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp37-none-any.whl size=12621 sha256=b3e6231a2874c6d96fe2a08d43d957035c4ced2a0a7819db8695c07af77d22f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "Successfully built gpustat\n",
            "Installing collected packages: py-spy, blessings, gpustat, colorful, opencensus-context, opencensus, multidict, yarl, async-timeout, aiohttp, aiohttp-cors, colorama, hiredis, aioredis, redis, ray\n",
            "Successfully installed aiohttp-3.7.4 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 gpustat-0.6.0 hiredis-1.1.0 multidict-5.1.0 opencensus-0.7.12 opencensus-context-0.1.2 py-spy-0.3.4 ray-1.0.0 redis-3.4.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uU-pjz0n6xpy",
        "outputId": "fc5ffa4a-77f1-4b78-d02a-25c86ac41c24"
      },
      "source": [
        "import or_gym\r\n",
        "from or_gym.utils import create_env\r\n",
        "import ray\r\n",
        "from ray.rllib import agents\r\n",
        "from ray import tune"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2pjjc-3KiWc"
      },
      "source": [
        "def register_env(env_name, env_config={}):\r\n",
        "    env = create_env(env_name)\r\n",
        "    tune.register_env(env_name, \r\n",
        "        lambda env_name: env(env_name,\r\n",
        "            env_config=env_config))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUYJHLpmKb7z"
      },
      "source": [
        "# Environment and RL Configuration Settings\r\n",
        "env_name = 'InvManagement-v1'\r\n",
        "env_config = {} # Change environment parameters here\r\n",
        "rl_config = dict(\r\n",
        "    env=env_name,\r\n",
        "    num_workers=2,\r\n",
        "    env_config=env_config,\r\n",
        "    model=dict(\r\n",
        "        vf_share_layers=False,\r\n",
        "        fcnet_activation='elu',\r\n",
        "        fcnet_hiddens=[256, 256]\r\n",
        "    ),\r\n",
        "   # framework='torch',\r\n",
        "    lr=1e-5\r\n",
        ")\r\n",
        "# Register environment\r\n",
        "register_env(env_name, env_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "NAueB4fOKlsI",
        "outputId": "b0d39fd6-b8a7-4bfa-945f-7ecd64d4c44e"
      },
      "source": [
        "ray.init(ignore_reinit_error=True,)\r\n",
        "agent = agents.ppo.PPOTrainer(env=env_name,\r\n",
        "    config=rl_config)\r\n",
        "print('ok')\r\n",
        "results = []\r\n",
        "for i in range(500):\r\n",
        "    res = agent.train()\r\n",
        "    results.append(res)\r\n",
        "    if (i+1) % 5 == 0:\r\n",
        "        print('\\rIter: {}\\tReward: {:.2f}'.format(\r\n",
        "                i+1, res['episode_reward_mean']), end='')\r\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-04 08:24:24,798\tINFO services.py:1166 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
            "2021-03-04 08:24:27,000\tINFO logger.py:201 -- pip install 'ray[tune]' to see TensorBoard files.\n",
            "2021-03-04 08:24:27,001\tWARNING logger.py:343 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
            "2021-03-04 08:24:27,009\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
            "2021-03-04 08:24:27,013\tINFO trainer.py:618 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m non-resource variables are not supported in the long term\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m 2021-03-04 08:24:31,414\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m 2021-03-04 08:24:31,463\tWARNING compression.py:16 -- lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
            "2021-03-04 08:24:38,790\tINFO trainable.py:255 -- Trainable.setup took 11.782 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2021-03-04 08:24:38,792\tWARNING util.py:39 -- Install gputil for GPU system monitoring.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ok\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/ray/rllib/policy/tf_policy.py:874: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/ray/rllib/policy/tf_policy.py:874: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=169)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/ray/rllib/policy/tf_policy.py:874: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(pid=170)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iter: 25\tReward: -13.56"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-37aa88327474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    789\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m                             \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/rllib/execution/train_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    206\u001b[0m                         batch_fetches = optimizer.optimize(\n\u001b[1;32m    207\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermutation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                             self.per_device_batch_size)\n\u001b[0m\u001b[1;32m    209\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEARNER_STATS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                             \u001b[0miter_extra_fetches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ray/rllib/execution/multi_gpu_impl.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, sess, batch_index)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mfetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtower\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_grad_and_stats_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_common_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1191\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1369\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1360\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuQlbhPgTeJB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}